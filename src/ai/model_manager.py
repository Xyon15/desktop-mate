"""
Model Manager pour Desktop-Mate (Kira)

Gestion du mod√®le LLM (Zephyr-7B) avec :
- Chargement mod√®le avec llama-cpp-python
- D√©tection GPU NVIDIA avec pynvml
- Application profils GPU adaptatifs
- G√©n√©ration texte avec contexte
- Gestion erreurs (OOM, mod√®le introuvable)
"""

import os
from typing import Optional, Dict, List, Any
import logging
from dataclasses import dataclass

# Import llama-cpp-python
try:
    from llama_cpp import Llama
    LLAMA_CPP_AVAILABLE = True
except ImportError:
    LLAMA_CPP_AVAILABLE = False
    Llama = None

# Import pynvml (GPU monitoring)
try:
    import pynvml
    PYNVML_AVAILABLE = True
except ImportError:
    PYNVML_AVAILABLE = False
    pynvml = None

from .config import AIConfig, get_config

logger = logging.getLogger(__name__)


@dataclass
class GPUInfo:
    """Informations sur le GPU NVIDIA"""
    available: bool
    name: Optional[str] = None
    vram_total: Optional[int] = None  # En bytes
    vram_free: Optional[int] = None   # En bytes
    vram_used: Optional[int] = None   # En bytes
    driver_version: Optional[str] = None
    cuda_version: Optional[str] = None


class ModelManager:
    """
    Gestionnaire du mod√®le LLM avec support GPU
    
    G√®re le cycle de vie du mod√®le :
    - D√©tection GPU
    - Chargement mod√®le avec profil GPU
    - G√©n√©ration texte
    - D√©chargement mod√®le
    """
    
    def __init__(self, config: Optional[AIConfig] = None):
        """
        Initialise le gestionnaire de mod√®le
        
        Args:
            config: Configuration IA (si None, charge depuis config.json)
        """
        self.config = config or get_config()
        self.model: Optional[Llama] = None
        self.is_loaded = False
        self.gpu_info: Optional[GPUInfo] = None
        
        # V√©rifier disponibilit√© llama-cpp-python
        if not LLAMA_CPP_AVAILABLE:
            logger.error(
                "‚ùå llama-cpp-python non disponible ! "
                "Installez-le avec : pip install llama-cpp-python"
            )
            raise ImportError("llama-cpp-python est requis pour ModelManager")
        
        logger.info("‚úÖ ModelManager initialis√©")
    
    def detect_gpu(self) -> GPUInfo:
        """
        D√©tecte le GPU NVIDIA et r√©cup√®re ses informations
        
        Returns:
            GPUInfo avec informations GPU (ou available=False si pas de GPU)
        """
        if not PYNVML_AVAILABLE:
            logger.warning(
                "‚ö†Ô∏è pynvml non disponible. Impossible de d√©tecter le GPU. "
                "Installez-le avec : pip install pynvml"
            )
            return GPUInfo(available=False)
        
        try:
            pynvml.nvmlInit()
            
            # Compter les GPUs
            device_count = pynvml.nvmlDeviceGetCount()
            
            if device_count == 0:
                logger.warning("‚ö†Ô∏è Aucun GPU NVIDIA d√©tect√©")
                return GPUInfo(available=False)
            
            # Prendre le premier GPU (index 0)
            handle = pynvml.nvmlDeviceGetHandleByIndex(0)
            
            # R√©cup√©rer infos
            name = pynvml.nvmlDeviceGetName(handle)
            memory_info = pynvml.nvmlDeviceGetMemoryInfo(handle)
            driver_version = pynvml.nvmlSystemGetDriverVersion()
            
            # CUDA version (peut √©chouer sur certains syst√®mes)
            try:
                cuda_version = pynvml.nvmlSystemGetCudaDriverVersion()
                cuda_version_str = f"{cuda_version // 1000}.{(cuda_version % 1000) // 10}"
            except:
                cuda_version_str = "Unknown"
            
            gpu_info = GPUInfo(
                available=True,
                name=name,
                vram_total=memory_info.total,
                vram_free=memory_info.free,
                vram_used=memory_info.used,
                driver_version=driver_version,
                cuda_version=cuda_version_str
            )
            
            logger.info(
                f"üéÆ GPU d√©tect√© : {name} | "
                f"VRAM: {memory_info.total / (1024**3):.1f} GB "
                f"(Libre: {memory_info.free / (1024**3):.1f} GB)"
            )
            
            pynvml.nvmlShutdown()
            
            self.gpu_info = gpu_info
            return gpu_info
            
        except Exception as e:
            logger.error(f"‚ùå Erreur d√©tection GPU : {e}")
            return GPUInfo(available=False)
    
    def load_model(self, force_profile: Optional[str] = None) -> bool:
        """
        Charge le mod√®le LLM avec le profil GPU configur√©
        
        Args:
            force_profile: Force un profil sp√©cifique (ignore config)
        
        Returns:
            True si chargement r√©ussi
        
        Raises:
            FileNotFoundError: Si le mod√®le n'existe pas
            RuntimeError: Si erreur de chargement (OOM, etc.)
        """
        if self.is_loaded:
            logger.warning("‚ö†Ô∏è Mod√®le d√©j√† charg√©. Utilisez unload_model() d'abord.")
            return True
        
        # V√©rifier existence du mod√®le
        model_path = self.config.model_path
        if not os.path.exists(model_path):
            error_msg = f"Mod√®le introuvable : {model_path}"
            logger.error(f"‚ùå {error_msg}")
            raise FileNotFoundError(error_msg)
        
        # D√©tecter GPU
        gpu_info = self.detect_gpu()
        
        # Choisir profil
        if force_profile:
            self.config.switch_profile(force_profile)
        
        profile_name = self.config.gpu_profile
        gpu_params = self.config.get_gpu_params()
        
        logger.info(
            f"üîÑ Chargement mod√®le : {os.path.basename(model_path)} "
            f"(profil: {profile_name})"
        )
        
        try:
            # Charger mod√®le avec llama-cpp-python
            self.model = Llama(
                model_path=model_path,
                n_gpu_layers=gpu_params["n_gpu_layers"],
                n_ctx=gpu_params["n_ctx"],
                n_batch=gpu_params["n_batch"],
                n_threads=gpu_params["n_threads"],
                use_mlock=gpu_params["use_mlock"],
                verbose=False  # D√©sactiver logs verbeux
            )
            
            self.is_loaded = True
            
            logger.info(
                f"‚úÖ Mod√®le charg√© avec succ√®s ! "
                f"(profil: {profile_name}, "
                f"GPU layers: {gpu_params['n_gpu_layers']}, "
                f"context: {gpu_params['n_ctx']})"
            )
            
            return True
            
        except Exception as e:
            error_msg = str(e)
            
            # D√©tecter erreur OOM (Out Of Memory)
            if "out of memory" in error_msg.lower() or "oom" in error_msg.lower():
                logger.error(
                    f"‚ùå Erreur VRAM insuffisante ! "
                    f"Profil actuel : {profile_name}. "
                    f"Essayez un profil moins gourmand (balanced ou cpu_fallback)."
                )
                
                # Auto-fallback vers CPU si erreur OOM
                if profile_name != "cpu_fallback":
                    logger.warning("‚ö†Ô∏è Tentative de fallback vers cpu_fallback...")
                    return self.load_model(force_profile="cpu_fallback")
            
            logger.error(f"‚ùå Erreur chargement mod√®le : {error_msg}")
            raise RuntimeError(f"√âchec chargement mod√®le : {error_msg}")
    
    def unload_model(self):
        """D√©charge le mod√®le de la m√©moire"""
        if not self.is_loaded:
            logger.warning("‚ö†Ô∏è Aucun mod√®le charg√©")
            return
        
        self.model = None
        self.is_loaded = False
        
        logger.info("‚úÖ Mod√®le d√©charg√©")
    
    def generate(
        self,
        prompt: str,
        temperature: Optional[float] = None,
        top_p: Optional[float] = None,
        max_tokens: Optional[int] = None,
        stop: Optional[List[str]] = None
    ) -> str:
        """
        G√©n√®re une r√©ponse texte √† partir d'un prompt
        
        Args:
            prompt: Texte d'entr√©e (peut contenir historique + question)
            temperature: Cr√©ativit√© (0.0-2.0). Si None, utilise config.
            top_p: Nucleus sampling (0.0-1.0). Si None, utilise config.
            max_tokens: Nombre max de tokens g√©n√©r√©s. Si None, utilise config.
            stop: Liste de s√©quences d'arr√™t (ex: ["\n\n", "User:"])
        
        Returns:
            Texte g√©n√©r√© par le mod√®le
        
        Raises:
            RuntimeError: Si le mod√®le n'est pas charg√©
        """
        if not self.is_loaded or self.model is None:
            raise RuntimeError(
                "Mod√®le non charg√© ! Appelez load_model() d'abord."
            )
        
        # Utiliser param√®tres config si non sp√©cifi√©s
        temperature = temperature if temperature is not None else self.config.temperature
        top_p = top_p if top_p is not None else self.config.top_p
        max_tokens = max_tokens if max_tokens is not None else self.config.max_tokens
        
        logger.debug(
            f"ü§ñ G√©n√©ration : "
            f"temp={temperature}, top_p={top_p}, max_tokens={max_tokens}"
        )
        
        try:
            # G√©n√©rer avec llama-cpp-python
            response = self.model(
                prompt,
                temperature=temperature,
                top_p=top_p,
                max_tokens=max_tokens,
                stop=stop or [],
                echo=False  # Ne pas r√©p√©ter le prompt dans la sortie
            )
            
            # Extraire le texte g√©n√©r√©
            generated_text = response["choices"][0]["text"].strip()
            
            logger.debug(f"‚úÖ G√©n√©ration termin√©e : {len(generated_text)} caract√®res")
            
            return generated_text
            
        except Exception as e:
            logger.error(f"‚ùå Erreur g√©n√©ration : {e}")
            raise RuntimeError(f"√âchec g√©n√©ration : {e}")
    
    def get_gpu_status(self) -> Dict[str, Any]:
        """
        R√©cup√®re le statut actuel du GPU
        
        Returns:
            Dictionnaire avec infos GPU actuelles
        """
        if not PYNVML_AVAILABLE:
            return {"available": False, "error": "pynvml non install√©"}
        
        try:
            pynvml.nvmlInit()
            handle = pynvml.nvmlDeviceGetHandleByIndex(0)
            
            memory_info = pynvml.nvmlDeviceGetMemoryInfo(handle)
            utilization = pynvml.nvmlDeviceGetUtilizationRates(handle)
            temperature = pynvml.nvmlDeviceGetTemperature(handle, pynvml.NVML_TEMPERATURE_GPU)
            
            status = {
                "available": True,
                "vram_used_gb": memory_info.used / (1024**3),
                "vram_free_gb": memory_info.free / (1024**3),
                "vram_total_gb": memory_info.total / (1024**3),
                "vram_percent": (memory_info.used / memory_info.total) * 100,
                "gpu_utilization_percent": utilization.gpu,
                "temperature_celsius": temperature
            }
            
            pynvml.nvmlShutdown()
            
            return status
            
        except Exception as e:
            logger.error(f"‚ùå Erreur r√©cup√©ration statut GPU : {e}")
            return {"available": False, "error": str(e)}
    
    def get_model_info(self) -> Dict[str, Any]:
        """
        R√©cup√®re les informations sur le mod√®le charg√©
        
        Returns:
            Dictionnaire avec infos mod√®le
        """
        return {
            "is_loaded": self.is_loaded,
            "model_path": self.config.model_path,
            "model_name": os.path.basename(self.config.model_path),
            "gpu_profile": self.config.gpu_profile,
            "gpu_params": self.config.get_gpu_params() if self.is_loaded else None,
            "gpu_info": {
                "available": self.gpu_info.available if self.gpu_info else False,
                "name": self.gpu_info.name if self.gpu_info else None,
                "vram_gb": (
                    self.gpu_info.vram_total / (1024**3)
                    if self.gpu_info and self.gpu_info.vram_total
                    else None
                )
            }
        }
    
    def __repr__(self) -> str:
        """Repr√©sentation string du ModelManager"""
        status = "charg√©" if self.is_loaded else "d√©charg√©"
        model_name = os.path.basename(self.config.model_path)
        return f"ModelManager({model_name}, {status}, profil={self.config.gpu_profile})"


# Instance globale (optionnel, pour usage singleton)
_model_manager_instance: Optional[ModelManager] = None


def get_model_manager(config: Optional[AIConfig] = None) -> ModelManager:
    """
    R√©cup√®re l'instance globale de ModelManager (singleton)
    
    Args:
        config: Configuration IA (optionnel)
    
    Returns:
        Instance ModelManager
    """
    global _model_manager_instance
    
    if _model_manager_instance is None:
        _model_manager_instance = ModelManager(config)
    
    return _model_manager_instance


# Pour tests et usage direct
if __name__ == "__main__":
    # Test rapide du ModelManager
    print("üß™ Test du ModelManager...\n")
    
    # Test 1 : D√©tection GPU
    print("1. D√©tection GPU...")
    manager = ModelManager()
    gpu_info = manager.detect_gpu()
    
    if gpu_info.available:
        print(f"   ‚úÖ GPU d√©tect√© : {gpu_info.name}")
        print(f"   VRAM : {gpu_info.vram_total / (1024**3):.1f} GB")
        print(f"   Driver : {gpu_info.driver_version}")
    else:
        print("   ‚ö†Ô∏è Aucun GPU d√©tect√© (mode CPU)")
    
    print()
    
    # Test 2 : Info mod√®le (sans charger)
    print("2. Informations mod√®le...")
    info = manager.get_model_info()
    print(f"   Mod√®le : {info['model_name']}")
    print(f"   Profil : {info['gpu_profile']}")
    print(f"   Charg√© : {info['is_loaded']}")
    
    print()
    
    # Test 3 : Chargement mod√®le (optionnel - peut √™tre long)
    print("3. Test chargement mod√®le...")
    print("   (D√©commentez le code ci-dessous pour tester)")
    print("   ‚ö†Ô∏è Attention : Le chargement prend 20-30 secondes\n")
    
    # D√©commenter pour tester le chargement complet :
    # try:
    #     manager.load_model()
    #     print("   ‚úÖ Mod√®le charg√© avec succ√®s !")
    #     
    #     # Test 4 : G√©n√©ration simple
    #     print("\n4. Test g√©n√©ration...")
    #     response = manager.generate("Dis bonjour en une phrase.")
    #     print(f"   R√©ponse : {response}")
    #     
    #     # D√©charger
    #     manager.unload_model()
    #     print("\n   ‚úÖ Mod√®le d√©charg√©")
    #     
    # except Exception as e:
    #     print(f"   ‚ùå Erreur : {e}")
    
    print("‚úÖ Tests manuels termin√©s !")
