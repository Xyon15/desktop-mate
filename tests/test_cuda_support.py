"""
Test script to verify CUDA support in llama-cpp-python
"""

import sys

print("=" * 60)
print("üß™ Test du support CUDA dans llama-cpp-python")
print("=" * 60)

try:
    import llama_cpp
    print(f"‚úÖ llama-cpp-python version: {llama_cpp.__version__}")
except ImportError as e:
    print(f"‚ùå Erreur d'import: {e}")
    sys.exit(1)

# Test 1: Check CUDA support
print("\nüìã Test 1: V√©rification support CUDA")
has_cuda = hasattr(llama_cpp.llama_cpp, 'llama_backend_cuda_init')
print(f"   CUDA support: {'‚úÖ OUI' if has_cuda else '‚ùå NON'}")

# Test 2: Check backend info
print("\nüìã Test 2: Informations backend")
try:
    # Try to get backend info
    from llama_cpp import llama_cpp
    
    # Check for CUDA-specific functions
    cuda_functions = [
        'llama_backend_cuda_init',
        'llama_backend_cuda_free',
    ]
    
    found_funcs = []
    for func_name in cuda_functions:
        if hasattr(llama_cpp, func_name):
            found_funcs.append(func_name)
    
    if found_funcs:
        print(f"   ‚úÖ Fonctions CUDA trouv√©es: {len(found_funcs)}")
        for func in found_funcs:
            print(f"      - {func}")
    else:
        print("   ‚ùå Aucune fonction CUDA trouv√©e")
        
except Exception as e:
    print(f"   ‚ö†Ô∏è Impossible de v√©rifier: {e}")

# Test 3: Try to load a model with GPU layers
print("\nüìã Test 3: Test chargement mod√®le avec GPU layers")
print("   (N√©cessite le mod√®le Zephyr-7B)")

import os
model_path = "models/zephyr-7b-beta.Q5_K_M.gguf"

if os.path.exists(model_path):
    print(f"   ‚úÖ Mod√®le trouv√©: {model_path}")
    
    try:
        print("   ‚è≥ Tentative de chargement avec 5 GPU layers...")
        from llama_cpp import Llama
        
        model = Llama(
            model_path=model_path,
            n_gpu_layers=5,  # Juste 5 couches pour test rapide
            n_ctx=512,       # Contexte r√©duit pour test rapide
            verbose=True     # Voir les logs d√©taill√©s
        )
        
        print("   ‚úÖ Mod√®le charg√© avec succ√®s !")
        
        # V√©rifier si GPU est vraiment utilis√©
        print("\n   üìä V√©rification utilisation GPU...")
        
        # Test simple de g√©n√©ration
        print("   ‚è≥ Test g√©n√©ration...")
        output = model("Hello", max_tokens=10, echo=False)
        print(f"   ‚úÖ G√©n√©ration r√©ussie: {output['choices'][0]['text'][:50]}...")
        
        # Cleanup
        del model
        print("   ‚úÖ Mod√®le d√©charg√©")
        
    except Exception as e:
        print(f"   ‚ùå Erreur lors du chargement: {e}")
        import traceback
        traceback.print_exc()
else:
    print(f"   ‚ö†Ô∏è Mod√®le non trouv√©: {model_path}")
    print("   Skipping test...")

print("\n" + "=" * 60)
print("üéØ R√©sum√©:")
if has_cuda:
    print("‚úÖ llama-cpp-python est compil√© AVEC support CUDA")
    print("‚úÖ Le mod√®le peut utiliser la VRAM du GPU NVIDIA")
else:
    print("‚ùå llama-cpp-python est compil√© SANS support CUDA")
    print("‚ùå Le mod√®le utilisera uniquement le CPU (RAM)")
print("=" * 60)
