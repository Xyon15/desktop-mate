"""
Chat Engine pour Desktop-Mate (Kira)

Moteur conversationnel unifi√© qui orchestre :
- M√©moire conversationnelle (ConversationMemory)
- G√©n√©ration LLM (ModelManager)
- D√©tection √©motionnelle basique
- Construction prompts avec contexte
- Sauvegarde automatique des conversations
"""

import logging
from typing import Optional, Dict, List, Any
from dataclasses import dataclass

from .memory import ConversationMemory, get_memory
from .model_manager import ModelManager, get_model_manager
from .config import AIConfig, get_config

logger = logging.getLogger(__name__)


@dataclass
class ChatResponse:
    """R√©ponse du chat engine"""
    response: str                      # Texte g√©n√©r√© par le mod√®le
    emotion: str                       # √âmotion d√©tect√©e ('joy', 'angry', etc.)
    tokens_used: int                   # Nombre approximatif de tokens
    context_messages: int              # Nombre de messages dans le contexte
    processing_time: float             # Temps de traitement en secondes


class EmotionDetector:
    """
    D√©tecteur d'√©motions basique par mots-cl√©s
    
    Analyse le texte g√©n√©r√© et retourne l'√©motion dominante.
    Version simple mais efficace pour Desktop-Mate.
    """
    
    # Mots-cl√©s par √©motion (fran√ßais)
    EMOTION_KEYWORDS = {
        'joy': [
            'heureux', 'heureuse', 'content', 'contente', 'super', 'g√©nial', 
            'excellent', 'parfait', 'cool', 'top', 'joie', 'merveilleux',
            'üòä', 'üòÑ', 'üòÅ', 'üéâ', '‚ú®', 'ü•∞', 'üòç', 'ü§ó',
            'r√©joui', 'enchant√©', 'ravi', 'formidable', 'magnifique'
        ],
        'angry': [
            '√©nerv√©', '√©nerv√©e', 'col√®re', 'furieux', 'furieuse', 'agac√©',
            'irrit√©', 'f√¢ch√©', 'rage', 'm√©content', 'contrari√©', 'aga√ßant',
            'üò†', 'üò°', 'ü§¨', 'grrr', 'argh', 'pfff',
            'exasp√©r√©', 'frustr√©', 'indign√©', 'erreur', 'probl√®me'
        ],
        'sorrow': [
            'triste', 'd√©sol√©', 'd√©sol√©e', 'dommage', 'malheureusement',
            'h√©las', 'peine', 'chagrin', 'malheureux', 'm√©lancolique',
            'üò¢', 'üò≠', 'üòî', 'üòû', 'üòü',
            'navr√©', 'attrist√©', 'd√©√ßu', 'regret'
        ],
        'surprised': [
            'wow', 'incroyable', 'surprenant', '√©tonnant', 'ooh', 'waouh',
            'oh', 'ah', 'stup√©fait', '√©bahi', 'impressionnant', 'stup√©fiant',
            'üò≤', 'üòÆ', 'ü§Ø', 'üòØ', 'üò≥',
            'inattendu', 'extraordinaire', 'ahurissant', 'attendais pas'
        ],
        'fun': [
            'dr√¥le', 'lol', 'mdr', 'hilarant', 'rigolo', 'amusant',
            'marrant', 'comique', 'blague', 'humour', 'rire',
            'üòÜ', 'üòÇ', 'ü§£', 'üòÑ', 'haha', 'hehe', 'hihi',
            'comique', 'cocasse', 'plaisant'
        ],
        'neutral': [
            'ok', 'bien', 'voil√†', 'alors', 'donc', 'effectivement'
        ]
    }
    
    def analyze(self, text: str) -> str:
        """
        Analyse le texte et retourne l'√©motion dominante
        
        Args:
            text: Texte √† analyser (r√©ponse du bot)
        
        Returns:
            √âmotion d√©tect√©e : 'joy', 'angry', 'sorrow', 'surprised', 'fun', 'neutral'
        """
        if not text or not text.strip():
            return 'neutral'
        
        text_lower = text.lower()
        
        # Compter occurrences par √©motion
        emotion_scores = {}
        
        for emotion, keywords in self.EMOTION_KEYWORDS.items():
            if emotion == 'neutral':
                continue  # Ne pas compter neutral dans le scoring
            
            score = sum(1 for keyword in keywords if keyword in text_lower)
            
            if score > 0:
                emotion_scores[emotion] = score
        
        # Retourner √©motion dominante ou neutral
        if not emotion_scores:
            return 'neutral'
        
        dominant_emotion = max(emotion_scores.items(), key=lambda x: x[1])[0]
        
        logger.debug(
            f"üé≠ √âmotion d√©tect√©e : {dominant_emotion} "
            f"(scores: {emotion_scores})"
        )
        
        return dominant_emotion


class ChatEngine:
    """
    Moteur conversationnel unifi√© pour Desktop-Mate
    
    Orchestre la m√©moire, le mod√®le LLM et la d√©tection √©motionnelle
    pour g√©n√©rer des r√©ponses coh√©rentes et √©motionnelles.
    
    Utilisable par :
    - Interface GUI Desktop-Mate (source="desktop")
    - Bot Discord (source="discord")
    """
    
    def __init__(
        self,
        config: Optional[AIConfig] = None,
        memory: Optional[ConversationMemory] = None,
        model_manager: Optional[ModelManager] = None
    ):
        """
        Initialise le Chat Engine
        
        Args:
            config: Configuration IA (si None, charge depuis config.json)
            memory: Gestionnaire m√©moire (si None, utilise singleton)
            model_manager: Gestionnaire mod√®le (si None, utilise singleton)
        """
        self.config = config or get_config()
        self.memory = memory or get_memory()
        self.model_manager = model_manager or get_model_manager(self.config)
        self.emotion_detector = EmotionDetector()
        
        logger.info("‚úÖ ChatEngine initialis√©")
    
    def _build_prompt(
        self,
        user_input: str,
        history: List[Dict[str, Any]]
    ) -> str:
        """
        Construit le prompt complet avec system prompt + historique + question
        
        Args:
            user_input: Message actuel de l'utilisateur
            history: Historique des conversations (liste de dicts)
        
        Returns:
            Prompt format√© pour le mod√®le
        """
        # Format du prompt pour Zephyr-7B (format ChatML)
        prompt_parts = []
        
        # System prompt
        prompt_parts.append(f"<|system|>\n{self.config.system_prompt}</|system|>")
        
        # Historique des conversations
        for interaction in history:
            user_msg = interaction['user_input']
            bot_msg = interaction['bot_response']
            
            prompt_parts.append(f"<|user|>\n{user_msg}</|user|>")
            prompt_parts.append(f"<|assistant|>\n{bot_msg}</|assistant|>")
        
        # Question actuelle
        prompt_parts.append(f"<|user|>\n{user_input}</|user|>")
        prompt_parts.append("<|assistant|>")
        
        prompt = "\n".join(prompt_parts)
        
        logger.debug(
            f"üìù Prompt construit : {len(prompt)} caract√®res, "
            f"{len(history)} messages d'historique"
        )
        
        return prompt
    
    def chat(
        self,
        user_input: str,
        user_id: str = "desktop_user",
        source: str = "desktop"
    ) -> ChatResponse:
        """
        G√©n√®re une r√©ponse conversationnelle
        
        Args:
            user_input: Message de l'utilisateur
            user_id: ID utilisateur (Discord ID ou "desktop_user")
            source: Source du message ("desktop" ou "discord")
        
        Returns:
            ChatResponse avec r√©ponse, √©motion, stats
        
        Raises:
            RuntimeError: Si le mod√®le n'est pas charg√©
        """
        import time
        start_time = time.time()
        
        logger.info(
            f"üí¨ Chat request : user={user_id[:8]}..., "
            f"source={source}, input_len={len(user_input)}"
        )
        
        # V√©rifier que le mod√®le est charg√©
        if not self.model_manager.is_loaded:
            error_msg = (
                "Mod√®le LLM non charg√© ! "
                "Appelez model_manager.load_model() d'abord."
            )
            logger.error(f"‚ùå {error_msg}")
            raise RuntimeError(error_msg)
        
        # 1. R√©cup√©rer l'historique
        history = self.memory.get_history(
            user_id=user_id,
            limit=self.config.context_limit,
            source=source
        )
        
        # 2. Construire le prompt
        prompt = self._build_prompt(user_input, history)
        
        # 3. G√©n√©rer la r√©ponse
        try:
            response_text = self.model_manager.generate(
                prompt=prompt,
                temperature=self.config.temperature,
                top_p=self.config.top_p,
                max_tokens=self.config.max_tokens,
                stop=["<|user|>", "<|system|>"]  # Arr√™ter aux balises
            )
        except Exception as e:
            logger.error(f"‚ùå Erreur g√©n√©ration : {e}")
            raise RuntimeError(f"√âchec g√©n√©ration r√©ponse : {e}")
        
        # 4. Analyser l'√©motion
        emotion = self.emotion_detector.analyze(response_text)
        
        # 5. Sauvegarder l'interaction
        self.memory.save_interaction(
            user_id=user_id,
            source=source,
            user_input=user_input,
            bot_response=response_text,
            emotion=emotion
        )
        
        # 6. Calculer stats
        processing_time = time.time() - start_time
        tokens_used = len(response_text.split())  # Approximation
        
        logger.info(
            f"‚úÖ R√©ponse g√©n√©r√©e : {len(response_text)} chars, "
            f"√©motion={emotion}, temps={processing_time:.2f}s"
        )
        
        return ChatResponse(
            response=response_text,
            emotion=emotion,
            tokens_used=tokens_used,
            context_messages=len(history),
            processing_time=processing_time
        )
    
    def clear_user_history(
        self,
        user_id: str,
        source: Optional[str] = None
    ) -> int:
        """
        Efface l'historique d'un utilisateur
        
        Args:
            user_id: ID utilisateur
            source: Filtrer par source (optionnel)
        
        Returns:
            Nombre d'interactions supprim√©es
        """
        deleted = self.memory.clear_user_history(user_id, source)
        
        logger.info(
            f"üóëÔ∏è Historique effac√© : {deleted} interactions "
            f"pour {user_id[:8]}... (source={source or 'all'})"
        )
        
        return deleted
    
    def get_stats(self) -> Dict[str, Any]:
        """
        R√©cup√®re les statistiques globales
        
        Returns:
            Dictionnaire avec stats m√©moire + mod√®le + engine
        """
        memory_stats = self.memory.get_stats()
        model_info = self.model_manager.get_model_info()
        
        return {
            'memory': memory_stats,
            'model': model_info,
            'config': {
                'context_limit': self.config.context_limit,
                'gpu_profile': self.config.gpu_profile,
                'temperature': self.config.temperature,
                'max_tokens': self.config.max_tokens
            }
        }
    
    def __repr__(self) -> str:
        """Repr√©sentation string du ChatEngine"""
        status = "pr√™t" if self.model_manager.is_loaded else "mod√®le non charg√©"
        return (
            f"ChatEngine({status}, "
            f"context={self.config.context_limit}, "
            f"profile={self.config.gpu_profile})"
        )


# Instance globale (optionnel, pour usage singleton)
_chat_engine_instance: Optional[ChatEngine] = None


def get_chat_engine(
    config: Optional[AIConfig] = None,
    memory: Optional[ConversationMemory] = None,
    model_manager: Optional[ModelManager] = None
) -> ChatEngine:
    """
    R√©cup√®re l'instance globale de ChatEngine (singleton)
    
    Args:
        config: Configuration IA (optionnel)
        memory: Gestionnaire m√©moire (optionnel)
        model_manager: Gestionnaire mod√®le (optionnel)
    
    Returns:
        Instance ChatEngine
    """
    global _chat_engine_instance
    
    if _chat_engine_instance is None:
        _chat_engine_instance = ChatEngine(config, memory, model_manager)
    
    return _chat_engine_instance


# Pour tests et usage direct
if __name__ == "__main__":
    # Test rapide du ChatEngine
    print("üß™ Test du ChatEngine...\n")
    
    # Test 1 : Initialisation
    print("1. Initialisation ChatEngine...")
    engine = ChatEngine()
    print(f"   ‚úÖ {engine}\n")
    
    # Test 2 : D√©tection √©motion
    print("2. Test d√©tection √©motions...")
    detector = EmotionDetector()
    
    tests = [
        ("Je suis super content ! üòä", "joy"),
        ("C'est vraiment triste... üò¢", "sorrow"),
        ("Wow, c'est incroyable ! üò≤", "surprised"),
        ("Haha, trop dr√¥le ! üòÇ", "fun"),
        ("Je suis tr√®s en col√®re ! üò†", "angry"),
        ("Voil√†, c'est fait.", "neutral")
    ]
    
    for text, expected in tests:
        emotion = detector.analyze(text)
        status = "‚úÖ" if emotion == expected else "‚ùå"
        print(f"   {status} '{text[:30]}...' ‚Üí {emotion}")
    
    print()
    
    # Test 3 : Construction prompt
    print("3. Test construction prompt...")
    history = [
        {'user_input': 'Bonjour', 'bot_response': 'Salut !'},
        {'user_input': '√áa va ?', 'bot_response': 'Tr√®s bien merci !'}
    ]
    
    prompt = engine._build_prompt("Comment tu t'appelles ?", history)
    print(f"   ‚úÖ Prompt construit : {len(prompt)} caract√®res")
    print(f"   (Contient {prompt.count('<|user|>')} messages utilisateur)")
    
    print()
    
    # Test 4 : Chat complet (n√©cessite mod√®le charg√©)
    print("4. Test conversation compl√®te...")
    print("   ‚ö†Ô∏è N√©cessite mod√®le charg√© (d√©commentez ci-dessous)")
    print()
    
    # D√©commenter pour tester avec le vrai mod√®le :
    # try:
    #     engine.model_manager.load_model()
    #     
    #     response = engine.chat(
    #         user_input="Bonjour Kira, pr√©sente-toi en une phrase courte.",
    #         user_id="test_user",
    #         source="desktop"
    #     )
    #     
    #     print(f"   ‚úÖ R√©ponse : {response.response}")
    #     print(f"   üé≠ √âmotion : {response.emotion}")
    #     print(f"   ‚è±Ô∏è Temps : {response.processing_time:.2f}s")
    #     
    #     engine.model_manager.unload_model()
    #     
    # except Exception as e:
    #     print(f"   ‚ùå Erreur : {e}")
    
    print("‚úÖ Tests manuels termin√©s !")
