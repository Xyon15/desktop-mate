"""
Configuration IA pour Desktop-Mate (Kira)

Gestion de la configuration IA :
- Profils GPU adaptatifs (Performance, Balanced, CPU Fallback)
- Param√®tres LLM (temperature, top_p, max_tokens)
- System prompt personnalisable
- Chargement depuis config.json avec valeurs par d√©faut
"""

import json
import os
from typing import Dict, Any, Optional
from dataclasses import dataclass, field
import logging

logger = logging.getLogger(__name__)


# ============================================
# PROFILS GPU PR√âD√âFINIS
# ============================================

GPU_PROFILES = {
    "performance": {
        "name": "Performance",
        "description": "Maximum GPU - Tr√®s rapide, pour conversations courtes",
        "n_gpu_layers": -1,      # Toutes les couches sur GPU
        "n_ctx": 4096,           # Contexte maximum (4096 tokens)
        "n_batch": 512,          # Batch size √©lev√©
        "n_threads": 6,          # Threads CPU
        "use_mlock": True,       # Lock memory pour √©viter swap
        "vram_estimate": "5-5.5 GB",
        "speed_estimate": "25-35 tokens/sec",
        "recommended_for": "R√©ponses ultra-rapides, autres apps ferm√©es"
    },
    "balanced": {
        "name": "Balanced",
        "description": "√âquilibre GPU/CPU - Rapide et stable (D√âFAUT)",
        "n_gpu_layers": 35,      # 35 couches GPU sur 43 (Zephyr-7B)
        "n_ctx": 2048,           # Contexte standard (2048 tokens)
        "n_batch": 256,          # Batch size mod√©r√©
        "n_threads": 6,          # Threads CPU
        "use_mlock": True,       # Lock memory
        "vram_estimate": "3-4 GB",
        "speed_estimate": "15-25 tokens/sec",
        "recommended_for": "Usage quotidien, conversations longues"
    },
    "cpu_fallback": {
        "name": "CPU Fallback",
        "description": "CPU uniquement - Lent mais toujours fonctionnel",
        "n_gpu_layers": 0,       # Aucune couche GPU (tout sur CPU)
        "n_ctx": 2048,           # Contexte standard
        "n_batch": 128,          # Batch size r√©duit
        "n_threads": 8,          # Plus de threads CPU
        "use_mlock": False,      # Pas de memory lock
        "vram_estimate": "0 GB (RAM: 4-6 GB)",
        "speed_estimate": "2-5 tokens/sec",
        "recommended_for": "Fallback si erreur VRAM ou sans GPU NVIDIA"
    }
}


@dataclass
class AIConfig:
    """
    Configuration IA pour Desktop-Mate
    
    Attributes:
        model_path: Chemin vers le mod√®le LLM (gguf)
        context_limit: Nombre de messages d'historique √† inclure
        gpu_profile: Profil GPU ("performance", "balanced", "cpu_fallback")
        temperature: Cr√©ativit√© des r√©ponses (0.0-2.0)
        top_p: Nucleus sampling (0.0-1.0)
        max_tokens: Nombre maximum de tokens g√©n√©r√©s
        system_prompt: Prompt syst√®me d√©finissant la personnalit√© de Kira
    """
    
    model_path: str = "models/zephyr-7b-beta.Q5_K_M.gguf"
    context_limit: int = 10
    gpu_profile: str = "balanced"
    temperature: float = 0.7
    top_p: float = 0.9
    max_tokens: int = 512
    system_prompt: str = field(default="Tu es Kira, un assistant virtuel amical.")
    
    def __post_init__(self):
        """Validation apr√®s initialisation"""
        self.validate()
    
    @classmethod
    def from_json(cls, config_path: str = "data/config.json") -> "AIConfig":
        """
        Charge la configuration depuis un fichier JSON
        
        Args:
            config_path: Chemin vers config.json
        
        Returns:
            Instance AIConfig
        
        Raises:
            FileNotFoundError: Si config.json n'existe pas
            ValueError: Si la config est invalide
        """
        if not os.path.exists(config_path):
            logger.warning(
                f"‚ö†Ô∏è Fichier config non trouv√© : {config_path}. "
                f"Utilisation de la configuration par d√©faut."
            )
            return cls()
        
        try:
            with open(config_path, 'r', encoding='utf-8') as f:
                config_data = json.load(f)
            
            # Extraire la section "ai"
            ai_config = config_data.get("ai", {})
            
            if not ai_config:
                logger.warning(
                    "‚ö†Ô∏è Section 'ai' manquante dans config.json. "
                    "Utilisation de la configuration par d√©faut."
                )
                return cls()
            
            # Cr√©er instance avec les valeurs du JSON
            instance = cls(
                model_path=ai_config.get("model_path", cls.model_path),
                context_limit=ai_config.get("context_limit", cls.context_limit),
                gpu_profile=ai_config.get("gpu_profile", cls.gpu_profile),
                temperature=ai_config.get("temperature", cls.temperature),
                top_p=ai_config.get("top_p", cls.top_p),
                max_tokens=ai_config.get("max_tokens", cls.max_tokens),
                system_prompt=ai_config.get("system_prompt", cls.system_prompt)
            )
            
            logger.info(
                f"‚úÖ Configuration IA charg√©e depuis {config_path} "
                f"(profil: {instance.gpu_profile})"
            )
            
            return instance
            
        except json.JSONDecodeError as e:
            logger.error(f"‚ùå Erreur parsing JSON : {e}")
            logger.warning("Utilisation de la configuration par d√©faut.")
            return cls()
        except Exception as e:
            logger.error(f"‚ùå Erreur chargement config : {e}")
            logger.warning("Utilisation de la configuration par d√©faut.")
            return cls()
    
    def get_gpu_params(self) -> Dict[str, Any]:
        """
        Retourne les param√®tres GPU du profil actuel
        
        Returns:
            Dictionnaire avec param√®tres llama-cpp-python
        
        Raises:
            ValueError: Si le profil GPU n'existe pas
        """
        if self.gpu_profile not in GPU_PROFILES:
            logger.error(
                f"‚ùå Profil GPU invalide : {self.gpu_profile}. "
                f"Utilisation de 'balanced'."
            )
            self.gpu_profile = "balanced"
        
        profile = GPU_PROFILES[self.gpu_profile]
        
        # Extraire uniquement les param√®tres techniques (pas description, etc.)
        gpu_params = {
            "n_gpu_layers": profile["n_gpu_layers"],
            "n_ctx": profile["n_ctx"],
            "n_batch": profile["n_batch"],
            "n_threads": profile["n_threads"],
            "use_mlock": profile["use_mlock"]
        }
        
        logger.debug(
            f"üéÆ Param√®tres GPU ({self.gpu_profile}): "
            f"layers={gpu_params['n_gpu_layers']}, "
            f"ctx={gpu_params['n_ctx']}, "
            f"batch={gpu_params['n_batch']}"
        )
        
        return gpu_params
    
    def get_profile_info(self) -> Dict[str, Any]:
        """
        Retourne les informations compl√®tes du profil GPU actuel
        
        Returns:
            Dictionnaire avec toutes les infos du profil
        """
        if self.gpu_profile not in GPU_PROFILES:
            self.gpu_profile = "balanced"
        
        return GPU_PROFILES[self.gpu_profile]
    
    def validate(self) -> bool:
        """
        Valide les param√®tres de configuration
        
        Returns:
            True si valide
        
        Raises:
            ValueError: Si un param√®tre est invalide
        """
        # Validation du chemin mod√®le
        if not self.model_path:
            raise ValueError("model_path ne peut pas √™tre vide")
        
        # Validation context_limit
        if not isinstance(self.context_limit, int) or self.context_limit < 1:
            raise ValueError(
                f"context_limit doit √™tre un entier >= 1 (re√ßu: {self.context_limit})"
            )
        
        if self.context_limit > 50:
            logger.warning(
                f"‚ö†Ô∏è context_limit tr√®s √©lev√© ({self.context_limit}). "
                f"Risque de d√©passer la limite de tokens du contexte."
            )
        
        # Validation gpu_profile
        if self.gpu_profile not in GPU_PROFILES:
            raise ValueError(
                f"gpu_profile invalide : {self.gpu_profile}. "
                f"Valeurs valides : {list(GPU_PROFILES.keys())}"
            )
        
        # Validation temperature
        if not isinstance(self.temperature, (int, float)):
            raise ValueError(
                f"temperature doit √™tre un nombre (re√ßu: {type(self.temperature)})"
            )
        
        if self.temperature < 0.0 or self.temperature > 2.0:
            raise ValueError(
                f"temperature doit √™tre entre 0.0 et 2.0 (re√ßu: {self.temperature})"
            )
        
        # Validation top_p
        if not isinstance(self.top_p, (int, float)):
            raise ValueError(
                f"top_p doit √™tre un nombre (re√ßu: {type(self.top_p)})"
            )
        
        if self.top_p < 0.0 or self.top_p > 1.0:
            raise ValueError(
                f"top_p doit √™tre entre 0.0 et 1.0 (re√ßu: {self.top_p})"
            )
        
        # Validation max_tokens
        if not isinstance(self.max_tokens, int) or self.max_tokens < 1:
            raise ValueError(
                f"max_tokens doit √™tre un entier >= 1 (re√ßu: {self.max_tokens})"
            )
        
        if self.max_tokens > 2048:
            logger.warning(
                f"‚ö†Ô∏è max_tokens tr√®s √©lev√© ({self.max_tokens}). "
                f"G√©n√©ration peut √™tre longue."
            )
        
        # Validation system_prompt
        if not isinstance(self.system_prompt, str) or not self.system_prompt.strip():
            raise ValueError("system_prompt ne peut pas √™tre vide")
        
        logger.debug("‚úÖ Configuration valid√©e")
        return True
    
    def to_dict(self) -> Dict[str, Any]:
        """
        Convertit la configuration en dictionnaire
        
        Returns:
            Dictionnaire avec tous les param√®tres
        """
        return {
            "model_path": self.model_path,
            "context_limit": self.context_limit,
            "gpu_profile": self.gpu_profile,
            "temperature": self.temperature,
            "top_p": self.top_p,
            "max_tokens": self.max_tokens,
            "system_prompt": self.system_prompt
        }
    
    def save_to_json(self, config_path: str = "data/config.json"):
        """
        Sauvegarde la configuration dans un fichier JSON
        
        Args:
            config_path: Chemin vers config.json
        
        Note:
            Cette m√©thode met √† jour UNIQUEMENT la section "ai"
            dans config.json, pr√©servant les autres sections.
        """
        # Charger config existante
        if os.path.exists(config_path):
            with open(config_path, 'r', encoding='utf-8') as f:
                config_data = json.load(f)
        else:
            config_data = {}
        
        # Mettre √† jour section "ai"
        config_data["ai"] = self.to_dict()
        
        # Sauvegarder
        os.makedirs(os.path.dirname(config_path), exist_ok=True)
        with open(config_path, 'w', encoding='utf-8') as f:
            json.dump(config_data, f, indent=4, ensure_ascii=False)
        
        logger.info(f"üíæ Configuration IA sauvegard√©e dans {config_path}")
    
    def switch_profile(self, new_profile: str):
        """
        Change le profil GPU actuel
        
        Args:
            new_profile: Nouveau profil ("performance", "balanced", "cpu_fallback")
        
        Raises:
            ValueError: Si le profil n'existe pas
        """
        if new_profile not in GPU_PROFILES:
            raise ValueError(
                f"Profil GPU invalide : {new_profile}. "
                f"Valeurs valides : {list(GPU_PROFILES.keys())}"
            )
        
        old_profile = self.gpu_profile
        self.gpu_profile = new_profile
        
        logger.info(
            f"üîÑ Profil GPU chang√© : {old_profile} ‚Üí {new_profile} "
            f"({GPU_PROFILES[new_profile]['description']})"
        )
    
    def __repr__(self) -> str:
        """Repr√©sentation string de la config"""
        return (
            f"AIConfig(model={os.path.basename(self.model_path)}, "
            f"profile={self.gpu_profile}, "
            f"context={self.context_limit}, "
            f"temp={self.temperature})"
        )


# Instance globale (optionnel, pour usage singleton)
_config_instance: Optional[AIConfig] = None


def get_config(config_path: str = "data/config.json") -> AIConfig:
    """
    R√©cup√®re l'instance globale de AIConfig (singleton)
    
    Args:
        config_path: Chemin vers config.json
    
    Returns:
        Instance AIConfig
    """
    global _config_instance
    
    if _config_instance is None:
        _config_instance = AIConfig.from_json(config_path)
    
    return _config_instance


def list_profiles() -> Dict[str, Dict[str, Any]]:
    """
    Liste tous les profils GPU disponibles
    
    Returns:
        Dictionnaire avec tous les profils
    """
    return GPU_PROFILES


# Pour tests et usage direct
if __name__ == "__main__":
    # Test rapide du syst√®me de configuration
    print("üß™ Test du syst√®me de configuration IA...\n")
    
    # Test 1 : Cr√©ation avec valeurs par d√©faut
    print("1. Cr√©ation config par d√©faut...")
    config = AIConfig()
    print(f"   ‚úÖ {config}")
    print(f"   Profile info: {config.get_profile_info()['description']}\n")
    
    # Test 2 : Chargement depuis JSON
    print("2. Chargement depuis config.json...")
    config_from_json = AIConfig.from_json("data/config.json")
    print(f"   ‚úÖ {config_from_json}")
    print(f"   System prompt (d√©but): {config_from_json.system_prompt[:50]}...\n")
    
    # Test 3 : R√©cup√©ration param√®tres GPU
    print("3. Param√®tres GPU (balanced)...")
    gpu_params = config.get_gpu_params()
    print(f"   ‚úÖ {gpu_params}\n")
    
    # Test 4 : Switch profil
    print("4. Changement de profil...")
    config.switch_profile("performance")
    print(f"   ‚úÖ {config}")
    gpu_params_perf = config.get_gpu_params()
    print(f"   GPU layers: {gpu_params_perf['n_gpu_layers']}\n")
    
    # Test 5 : Liste tous les profils
    print("5. Liste des profils disponibles...")
    profiles = list_profiles()
    for name, info in profiles.items():
        print(f"   - {name}: {info['description']}")
    
    print("\n‚úÖ Tous les tests manuels pass√©s !")
