# üìñ Contexte pour Chat 7 - Desktop-Mate (Session 10 - Phase 3+)

**Date** : Octobre 2025  
**Session** : Session 10 - IA Conversationnelle (Kira)  
**Phase suivante** : Phase 3 - Configuration IA  
**Progression** : 2/14 phases (14%)

---

## üéØ Objectif Chat 7

**Compl√©ter les phases suivantes** :
- ‚úÖ **Phase 3** : Configuration IA (1h)
- ‚úÖ **Phase 4** : Model Manager (2-3h)
- ‚úÖ **Phase 5** : Chat Engine (2-3h)
- ‚ö†Ô∏è *Optionnel si temps* : Phase 6 - Emotion Analyzer (1-2h)

**Dur√©e estim√©e** : 5-8h (ou jusqu'√† 10h avec Phase 6)

---

## üìö Documents √† Lire AVANT de Commencer

### 1Ô∏è‚É£ Transition Chat 6 ‚Üí Chat 7

**OBLIGATOIRES** :
- ‚úÖ `docs/chat_transitions/chat_6_session_10_phases_1_2/CURRENT_STATE.md`  
  ‚Üí √âtat technique complet apr√®s Phases 1-2
  
- ‚úÖ `docs/chat_transitions/chat_6_session_10_phases_1_2/CHAT_SUMMARY.md`  
  ‚Üí R√©sum√© des r√©alisations Chat 6

- ‚úÖ Ce fichier : `CONTEXT_FOR_NEXT_CHAT.md`  
  ‚Üí Contexte et instructions pour Chat 7

### 2Ô∏è‚É£ Documentation Session 10

**OBLIGATOIRES** :
- ‚úÖ `docs/sessions/session_10_ai_chat/PLAN_SESSION_10.md`  
  ‚Üí **CRITIQUE** : Plan complet 14 phases avec d√©tails Phase 3-5
  
- ‚úÖ `docs/sessions/session_10_ai_chat/README.md`  
  ‚Üí Vue d'ensemble Session 10 et progression

### 3Ô∏è‚É£ Instructions Copilot

**OBLIGATOIRES** :
- ‚úÖ `.github/instructions/copilot-instructions.instructions.md`  
  ‚Üí R√®gles de documentation, workflow, communication

---

## üß† Ce que tu Dois Savoir du Chat 6

### ‚úÖ Ce qui a √©t√© fait (Phases 1-2)

#### Phase 1 : Architecture de Base
- ‚úÖ Dossiers cr√©√©s : `src/ai/`, `src/discord_bot/`, `src/auth/`, `models/`
- ‚úÖ Fichiers `__init__.py` pour tous les modules
- ‚úÖ Mod√®le LLM copi√© : `models/zephyr-7b-beta.Q5_K_M.gguf` (6.8 GB)
- ‚úÖ Configuration : `.env`, `.env.example`, `.gitignore`, `requirements.txt`
- ‚úÖ Documentation : `README.md`, `PLAN_SESSION_10.md`, `INDEX.md` mis √† jour

#### Phase 2 : Base de Donn√©es & M√©moire
- ‚úÖ `src/ai/memory.py` (430 lignes) - Syst√®me conversationnel complet
- ‚úÖ SQLite `data/chat_history.db` avec 4 indexes optimis√©s
- ‚úÖ Tests `tests/test_memory.py` - 11/11 tests passent ‚úÖ
- ‚úÖ Singleton pattern avec `get_memory()`
- ‚úÖ Context manager thread-safe

### üîë Informations Critiques

**Mod√®le LLM** :
- Fichier : `models/zephyr-7b-beta.Q5_K_M.gguf`
- Taille : 6.8 GB
- Quantization : Q5_K_M (excellent pour RTX 4050)
- Performance : ~20-30 tokens/sec

**Base de donn√©es** :
- SQLite : `data/chat_history.db`
- Table : `chat_history` (7 colonnes)
- Indexes : 4 (user_id, source, timestamp, user_timestamp)

**GPU Utilisateur** :
- NVIDIA RTX 4050 (6 GB VRAM)
- Profils pr√©vus : Performance, Balanced, CPU Fallback

**Variables d'environnement** :
- `.env` configur√© avec `DISCORD_TOKEN`
- `.env.example` comme template

### üì¶ Modules Op√©rationnels

```python
# Syst√®me de m√©moire (FONCTIONNEL)
from src.ai.memory import ConversationMemory, get_memory

memory = get_memory()  # Singleton
memory.save_interaction(user_id, source, user_input, bot_response, emotion)
history = memory.get_history(user_id, limit=10)
stats = memory.get_stats()
```

**Tests** :
```powershell
pytest tests/test_memory.py -v
# 11 passed in 0.70s ‚úÖ
```

### ‚ö†Ô∏è Ce qui N'est PAS fait

- ‚è≥ Configuration IA centralis√©e
- ‚è≥ Gestion LLM (chargement, g√©n√©ration)
- ‚è≥ Moteur conversationnel
- ‚è≥ Analyse √©motionnelle
- ‚è≥ Bot Discord
- ‚è≥ Interface GUI chat
- ‚è≥ Syst√®me 2FA
- ‚è≥ Int√©gration Unity IPC

---

## üéØ Phase 3 : Configuration IA (PROCHAINE)

### Objectif Phase 3

Cr√©er un syst√®me de configuration centralis√© pour l'IA avec :
- ‚úÖ Profils GPU adaptatifs
- ‚úÖ Param√®tres LLM configurables
- ‚úÖ System prompt personnalisable
- ‚úÖ Validation des param√®tres

### Fichiers √† Cr√©er

#### 1Ô∏è‚É£ `src/ai/config.py`

**Contenu attendu** :
```python
"""
Configuration IA pour Desktop-Mate
Gestion profils GPU, param√®tres LLM, system prompt
"""

import json
from typing import Dict, Any, Optional
from dataclasses import dataclass
import logging

logger = logging.getLogger(__name__)

# Profils GPU pr√©d√©finis
GPU_PROFILES = {
    "performance": {
        "n_gpu_layers": -1,      # Toutes layers sur GPU
        "n_ctx": 4096,           # Contexte max
        "n_batch": 512,          # Batch size max
        "description": "Max GPU, id√©al conversations courtes"
    },
    "balanced": {
        "n_gpu_layers": 35,      # 35 layers GPU (Zephyr-7B)
        "n_ctx": 2048,           # Contexte mod√©r√©
        "n_batch": 256,          # Batch mod√©r√©
        "description": "√âquilibre GPU/CPU (D√âFAUT)"
    },
    "cpu_fallback": {
        "n_gpu_layers": 0,       # CPU uniquement
        "n_ctx": 2048,
        "n_batch": 128,
        "description": "Fallback sans GPU"
    }
}

@dataclass
class AIConfig:
    """Configuration IA"""
    model_path: str
    context_limit: int
    gpu_profile: str
    temperature: float
    top_p: float
    max_tokens: int
    system_prompt: str
    
    @classmethod
    def from_json(cls, config_path: str = "data/config.json") -> "AIConfig":
        """Charge config depuis JSON"""
        # Charger config.json
        # Extraire section "ai"
        # Valider param√®tres
        # Retourner AIConfig
        pass
    
    def get_gpu_params(self) -> Dict[str, Any]:
        """Retourne param√®tres GPU du profil"""
        return GPU_PROFILES.get(self.gpu_profile, GPU_PROFILES["balanced"])
    
    def validate(self) -> bool:
        """Valide les param√®tres"""
        # V√©rifier model_path existe
        # V√©rifier gpu_profile valide
        # V√©rifier temperature/top_p dans [0, 1]
        # V√©rifier max_tokens > 0
        pass
```

**Voir d√©tails complets** : `PLAN_SESSION_10.md` Phase 3

#### 2Ô∏è‚É£ √âtendre `data/config.json`

**Ajouter section IA** :
```json
{
  "existing_config": "...",
  
  "ai": {
    "model_path": "models/zephyr-7b-beta.Q5_K_M.gguf",
    "context_limit": 2048,
    "gpu_profile": "balanced",
    "temperature": 0.7,
    "top_p": 0.9,
    "max_tokens": 512,
    "system_prompt": "Tu es Kira, l'assistante IA de Desktop-Mate. Tu es amicale, serviable et expressive. Tu peux ressentir et exprimer des √©motions √† travers tes expressions faciales. R√©ponds de mani√®re naturelle et engageante."
  }
}
```

#### 3Ô∏è‚É£ Tests : `tests/test_config.py`

**Tests √† cr√©er** :
- ‚úÖ `test_load_config()` - Chargement config.json
- ‚úÖ `test_gpu_profiles()` - Profils GPU valides
- ‚úÖ `test_validation()` - Validation param√®tres
- ‚úÖ `test_default_values()` - Valeurs par d√©faut
- ‚úÖ `test_invalid_profile()` - Gestion erreurs

### Dur√©e Phase 3

**Estim√©e** : 1h

**D√©composition** :
- 30 min : `src/ai/config.py` + `GPU_PROFILES`
- 15 min : √âtendre `data/config.json`
- 15 min : Tests `test_config.py`

---

## üöÄ Phase 4 : Model Manager (APR√àS Phase 3)

### Objectif Phase 4

Cr√©er le gestionnaire LLM qui :
- ‚úÖ Charge le mod√®le avec llama-cpp-python
- ‚úÖ D√©tecte GPU avec pynvml
- ‚úÖ Applique profil GPU appropri√©
- ‚úÖ G√©n√®re texte avec contexte
- ‚úÖ G√®re erreurs (OOM, model not found)

### Fichier √† Cr√©er

#### `src/ai/model_manager.py`

**Contenu attendu** :
```python
"""
Model Manager pour Desktop-Mate
Gestion LLM (llama-cpp-python), GPU, g√©n√©ration texte
"""

from llama_cpp import Llama
import pynvml
from typing import Optional, Dict, List
from .config import AIConfig
import logging

logger = logging.getLogger(__name__)

class ModelManager:
    """Gestionnaire du mod√®le LLM"""
    
    def __init__(self, config: AIConfig):
        self.config = config
        self.model: Optional[Llama] = None
        self.gpu_available = self._check_gpu()
        
    def _check_gpu(self) -> bool:
        """D√©tecte GPU NVIDIA"""
        try:
            pynvml.nvmlInit()
            device_count = pynvml.nvmlDeviceGetCount()
            if device_count > 0:
                handle = pynvml.nvmlDeviceGetHandleByIndex(0)
                name = pynvml.nvmlDeviceGetName(handle)
                logger.info(f"GPU d√©tect√© : {name}")
                return True
        except Exception as e:
            logger.warning(f"GPU non d√©tect√© : {e}")
        return False
    
    def load_model(self) -> bool:
        """Charge le mod√®le LLM"""
        # R√©cup√©rer param√®tres GPU du profil
        # Cr√©er Llama avec llama-cpp-python
        # G√©rer erreurs (OOM, fichier absent)
        pass
    
    def generate(self, prompt: str, max_tokens: Optional[int] = None) -> str:
        """G√©n√®re texte depuis prompt"""
        # Utiliser self.model(prompt, ...)
        # Extraire texte g√©n√©r√©
        # G√©rer erreurs
        pass
    
    def get_gpu_info(self) -> Dict:
        """Retourne info GPU (VRAM, temp√©rature)"""
        pass
```

**Voir d√©tails complets** : `PLAN_SESSION_10.md` Phase 4

### Tests : `tests/test_model_manager.py`

**Tests √† cr√©er** :
- ‚úÖ `test_gpu_detection()` - D√©tection GPU
- ‚úÖ `test_load_model()` - Chargement mod√®le
- ‚úÖ `test_generate()` - G√©n√©ration texte
- ‚úÖ `test_gpu_info()` - Info GPU
- ‚úÖ `test_error_handling()` - Gestion erreurs

### Dur√©e Phase 4

**Estim√©e** : 2-3h

---

## üé§ Phase 5 : Chat Engine (APR√àS Phase 4)

### Objectif Phase 5

Cr√©er le moteur conversationnel unifi√© qui :
- ‚úÖ Orchestre m√©moire + model manager
- ‚úÖ G√®re contexte conversations (historique)
- ‚úÖ Construit prompts avec system prompt
- ‚úÖ G√©n√®re r√©ponses coh√©rentes
- ‚úÖ D√©tecte √©motions (basique)

### Fichier √† Cr√©er

#### `src/ai/chat_engine.py`

**Contenu attendu** :
```python
"""
Chat Engine pour Desktop-Mate
Moteur conversationnel unifi√© (GUI + Discord)
"""

from typing import Optional, Dict
from .model_manager import ModelManager
from .memory import ConversationMemory, get_memory
from .config import AIConfig
import logging

logger = logging.getLogger(__name__)

class ChatEngine:
    """Moteur conversationnel unifi√©"""
    
    def __init__(self, config: AIConfig):
        self.config = config
        self.model_manager = ModelManager(config)
        self.memory = get_memory()
        
    def start(self) -> bool:
        """D√©marre le moteur (charge mod√®le)"""
        return self.model_manager.load_model()
    
    def chat(self, user_id: str, user_input: str, source: str = "desktop") -> Dict:
        """
        G√©n√®re r√©ponse pour user_input
        
        Returns:
            {
                "response": str,
                "emotion": str,
                "error": Optional[str]
            }
        """
        # 1. R√©cup√©rer historique
        # 2. Construire prompt avec system_prompt + historique + user_input
        # 3. G√©n√©rer r√©ponse
        # 4. D√©tecter √©motion (basique)
        # 5. Sauvegarder interaction
        # 6. Retourner dict
        pass
    
    def _build_prompt(self, history: List[Dict], user_input: str) -> str:
        """Construit prompt complet"""
        pass
    
    def _detect_emotion(self, text: str) -> str:
        """D√©tection √©motionnelle basique (keywords)"""
        # D√©tection simple par mots-cl√©s
        # Version am√©lior√©e en Phase 6
        pass
```

**Voir d√©tails complets** : `PLAN_SESSION_10.md` Phase 5

### Tests : `tests/test_chat_engine.py`

**Tests √† cr√©er** :
- ‚úÖ `test_chat_basic()` - Conversation basique
- ‚úÖ `test_chat_with_history()` - Avec contexte
- ‚úÖ `test_emotion_detection()` - D√©tection √©motion
- ‚úÖ `test_memory_integration()` - Int√©gration m√©moire
- ‚úÖ `test_error_handling()` - Gestion erreurs

### Dur√©e Phase 5

**Estim√©e** : 2-3h

---

## üìä Ordre d'Impl√©mentation Chat 7

### S√©quence Recommand√©e

```
1. Phase 3 : Configuration IA (1h)
   ‚îú‚îÄ‚îÄ src/ai/config.py
   ‚îú‚îÄ‚îÄ data/config.json (√©tendre)
   ‚îî‚îÄ‚îÄ tests/test_config.py

2. Phase 4 : Model Manager (2-3h)
   ‚îú‚îÄ‚îÄ src/ai/model_manager.py
   ‚îî‚îÄ‚îÄ tests/test_model_manager.py

3. Phase 5 : Chat Engine (2-3h)
   ‚îú‚îÄ‚îÄ src/ai/chat_engine.py
   ‚îî‚îÄ‚îÄ tests/test_chat_engine.py

4. [OPTIONNEL] Phase 6 : Emotion Analyzer (1-2h)
   ‚îú‚îÄ‚îÄ src/ai/emotion_analyzer.py
   ‚îî‚îÄ‚îÄ tests/test_emotion_analyzer.py
```

**Temps total estim√©** :
- Phases 3-5 : 5-7h
- Avec Phase 6 : 6-9h

---

## üß™ Strat√©gie de Tests

### Commandes Importantes

**Tester module sp√©cifique** :
```powershell
pytest tests/test_config.py -v
pytest tests/test_model_manager.py -v
pytest tests/test_chat_engine.py -v
```

**Tester tous les tests IA** :
```powershell
pytest tests/test_memory.py tests/test_config.py tests/test_model_manager.py tests/test_chat_engine.py -v
```

**Tests avec couverture** :
```powershell
pytest --cov=src.ai tests/ --cov-report=html
```

### Validation Apr√®s Chaque Phase

**Apr√®s Phase 3** :
```powershell
pytest tests/test_config.py -v
# V√©rifier chargement config.json OK
```

**Apr√®s Phase 4** :
```powershell
pytest tests/test_model_manager.py -v
# V√©rifier d√©tection GPU OK
# V√©rifier chargement mod√®le OK (peut √™tre long ~30s)
```

**Apr√®s Phase 5** :
```powershell
pytest tests/test_chat_engine.py -v
# V√©rifier conversation basique OK
# V√©rifier sauvegarde m√©moire OK
```

---

## üìö R√©f√©rences Utiles

### Documentation Externe

**llama-cpp-python** :
- Docs : https://llama-cpp-python.readthedocs.io/
- Param√®tres Llama : `n_gpu_layers`, `n_ctx`, `n_batch`, `temperature`, `top_p`

**pynvml** :
- Docs : https://pypi.org/project/pynvml/
- Fonctions : `nvmlInit()`, `nvmlDeviceGetCount()`, `nvmlDeviceGetHandleByIndex()`

**SQLite Python** :
- Docs : https://docs.python.org/3/library/sqlite3.html

### Code de R√©f√©rence (Kira-Bot)

**‚ö†Ô∏è IMPORTANT** : Kira-Bot existe mais est d√©sorganis√© (score 5.7/10)

**Utilise comme r√©f√©rence UNIQUEMENT** :
- `C:\Dev\IA-chatbot\model.py` - Exemple ModelManager
- `C:\Dev\IA-chatbot\config.py` - Exemple configuration
- `C:\Dev\IA-chatbot\memory.py` - Notre version est meilleure !

**NE PAS copier-coller** : Adapter les concepts, r√©√©crire proprement

---

## üîß Configuration Utilisateur

### Variables d'Environnement

**Fichier** : `.env` (configur√©)
```
DISCORD_TOKEN=<configur√©_par_utilisateur>
TOTP_SECRET=<sera_g√©n√©r√©_en_phase_10>
```

### Mat√©riel

**GPU** : NVIDIA RTX 4050 (6 GB VRAM)  
**Profil recommand√©** : `balanced` (d√©faut)  
**Performance attendue** : 20-30 tokens/sec

---

## üö® Points d'Attention

### 1. Chargement Mod√®le (Phase 4)

**‚ö†Ô∏è Peut √™tre lent** : Chargement initial ~20-30s pour 6.8 GB

**Solutions** :
- Afficher message "Chargement mod√®le..."
- Logger progression
- G√©rer timeout

### 2. Gestion M√©moire GPU (Phase 4)

**‚ö†Ô∏è Risque OOM** : Si profil trop √©lev√© pour GPU

**Solutions** :
- Commencer avec profil `balanced`
- D√©tecter erreur OOM
- Fallback automatique vers `cpu_fallback`
- Logger √©v√©nement

### 3. Contexte Conversations (Phase 5)

**‚ö†Ô∏è Limite contexte** : 2048 tokens (profil balanced)

**Solutions** :
- Limiter historique √† 10 derniers messages
- R√©sumer vieux messages (Phase future)
- Afficher warning si contexte plein

### 4. Tests Longs (Phase 4-5)

**‚ö†Ô∏è Tests avec LLM** : G√©n√©ration texte peut prendre 5-10s

**Solutions** :
- Utiliser `@pytest.mark.slow` pour tests LLM
- Permettre `pytest -m "not slow"` pour tests rapides
- Mock pour tests unitaires, int√©gration pour tests lents

---

## üìñ Documentation √† Mettre √† Jour

### Apr√®s CHAQUE Phase

**OBLIGATOIRE** :
- ‚úÖ `docs/INDEX.md` - Ajouter nouveaux fichiers
- ‚úÖ `docs/README.md` - Si architecture modifi√©e
- ‚úÖ `README.md` (racine) - Si fonctionnalit√© majeure
- ‚úÖ `docs/sessions/session_10_ai_chat/README.md` - Progression phases

**SYST√àME CRITIQUE** :
Suivre `.github/instructions/copilot-instructions.instructions.md`

**R√®gle d'or** :
> "L'utilisateur ne devrait JAMAIS avoir √† demander si la documentation est √† jour"

---

## üéØ Objectif Final Chat 7

√Ä la fin du Chat 7, Desktop-Mate devrait avoir :

‚úÖ Configuration IA centralis√©e et test√©e  
‚úÖ Mod√®le LLM chargeable avec profils GPU  
‚úÖ Moteur conversationnel fonctionnel  
‚úÖ G√©n√©ration de r√©ponses avec contexte  
‚úÖ Sauvegarde automatique des conversations  
‚úÖ D√©tection √©motionnelle basique  
‚úÖ Tests complets (15-20 tests)  
‚úÖ Documentation √† jour  

**Pr√™t pour** :
- Phase 7 : Bot Discord (Chat 8)
- Phase 8 : GUI Chat (Chat 8)

---

## üí° Conseils pour l'IA (Toi)

### Workflow Recommand√©

**Pour chaque phase** :
1. ‚úÖ Lire d√©tails dans `PLAN_SESSION_10.md`
2. ‚úÖ Expliquer ce que tu vas faire
3. ‚úÖ Cr√©er les fichiers Python
4. ‚úÖ Cr√©er les tests
5. ‚úÖ Ex√©cuter les tests
6. ‚úÖ Corriger les erreurs
7. ‚úÖ **METTRE √Ä JOUR DOCUMENTATION**
8. ‚úÖ Afficher r√©capitulatif phase

**JAMAIS dire "Termin√©" sans** :
- ‚úÖ Tests passants
- ‚úÖ Documentation mise √† jour
- ‚úÖ R√©capitulatif affich√©

### Communication avec Utilisateur

**Rappels** :
- üá´üá∑ Toujours en fran√ßais
- üìñ Expliquer clairement les concepts techniques
- ‚ö†Ô∏è Demander confirmation avant changements majeurs
- üéì L'utilisateur n'est pas expert ‚Üí √ätre p√©dagogue

### Gestion du Temps

**Phases longues (4-5)** :
- D√©couper en sous-t√¢ches
- Afficher progression r√©guli√®rement
- Permettre pauses si demand√©es

**Tests** :
- Ne pas attendre la fin pour tester
- Tester apr√®s chaque m√©thode critique
- Corriger au fur et √† mesure

---

## üîó Liens Rapides

**Documentation Session 10** :
- `docs/sessions/session_10_ai_chat/PLAN_SESSION_10.md` ‚Üê **CRITIQUE**
- `docs/sessions/session_10_ai_chat/README.md`

**Transition Chat 6** :
- `docs/chat_transitions/chat_6_session_10_phases_1_2/CURRENT_STATE.md`
- `docs/chat_transitions/chat_6_session_10_phases_1_2/CHAT_SUMMARY.md`

**Instructions Projet** :
- `.github/instructions/copilot-instructions.instructions.md`

**Code Existant** :
- `src/ai/memory.py` ‚Üê Exemple qualit√© attendue
- `tests/test_memory.py` ‚Üê Exemple tests complets

---

## üéä Message de Motivation

Chat 6 a pos√© des **bases solides** ! üéâ

L'architecture est propre, la m√©moire fonctionne parfaitement (11/11 tests ‚úÖ), et le mod√®le LLM est pr√™t.

**Chat 7 va donner vie √† Kira** en lui permettant de :
- üß† Penser (Model Manager)
- üí¨ Converser (Chat Engine)
- üé≠ Ressentir (Emotion basique)

**Tu es pr√™t √† cr√©er la partie la plus excitante du projet ! üöÄ**

---

**Bon d√©veloppement Chat 7 ! Tu as toutes les infos n√©cessaires ! üé≠‚ú®**
