# ğŸ“– Contexte pour Chat 7 - Desktop-Mate (Session 10 - Phase 3+)

**Date** : Octobre 2025  
**Session** : Session 10 - IA Conversationnelle (Kira)  
**Phase suivante** : Phase 3 - Configuration IA  
**Progression** : 2/14 phases (14%)

---

## ğŸ¯ Objectif Chat 7

**ComplÃ©ter les phases suivantes** :
- âœ… **Phase 3** : Configuration IA (1h)
- âœ… **Phase 4** : Model Manager (2-3h)
- âœ… **Phase 5** : Chat Engine (2-3h)
- âš ï¸ *Optionnel si temps* : Phase 6 - Emotion Analyzer (1-2h)

**DurÃ©e estimÃ©e** : 5-8h (ou jusqu'Ã  10h avec Phase 6)

---

## ğŸ“š Documents Ã  Lire AVANT de Commencer

### 1ï¸âƒ£ Transition Chat 6 â†’ Chat 7

**OBLIGATOIRES** :
- âœ… `docs/chat_transitions/chat_6_session_10_phases_1_2/CURRENT_STATE.md`  
  â†’ Ã‰tat technique complet aprÃ¨s Phases 1-2
  
- âœ… `docs/chat_transitions/chat_6_session_10_phases_1_2/CHAT_SUMMARY.md`  
  â†’ RÃ©sumÃ© des rÃ©alisations Chat 6

- âœ… Ce fichier : `CONTEXT_FOR_NEXT_CHAT.md`  
  â†’ Contexte et instructions pour Chat 7

### 2ï¸âƒ£ Documentation Session 10

**OBLIGATOIRES** :
- âœ… `docs/sessions/session_10_ai_chat/PLAN_SESSION_10.md`  
  â†’ **CRITIQUE** : Plan complet 14 phases avec dÃ©tails Phase 3-5
  
- âœ… `docs/sessions/session_10_ai_chat/README.md`  
  â†’ Vue d'ensemble Session 10 et progression

### 3ï¸âƒ£ Instructions Copilot

**OBLIGATOIRES** :
- âœ… `.github/instructions/copilot-instructions.instructions.md`  
  â†’ RÃ¨gles de documentation, workflow, communication

---

## ğŸ§  Ce que tu Dois Savoir du Chat 6

### âœ… Ce qui a Ã©tÃ© fait (Phases 1-2)

#### Phase 1 : Architecture de Base
- âœ… Dossiers crÃ©Ã©s : `src/ai/`, `src/discord_bot/`, `src/auth/`, `models/`
- âœ… Fichiers `__init__.py` pour tous les modules
- âœ… ModÃ¨le LLM copiÃ© : `models/zephyr-7b-beta.Q5_K_M.gguf` (6.8 GB)
- âœ… Configuration : `.env`, `.env.example`, `.gitignore`, `requirements.txt`
- âœ… Documentation : `README.md`, `PLAN_SESSION_10.md`, `INDEX.md` mis Ã  jour

#### Phase 2 : Base de DonnÃ©es & MÃ©moire
- âœ… `src/ai/memory.py` (430 lignes) - SystÃ¨me conversationnel complet
- âœ… SQLite `data/chat_history.db` avec 4 indexes optimisÃ©s
- âœ… Tests `tests/test_memory.py` - 11/11 tests passent âœ…
- âœ… Singleton pattern avec `get_memory()`
- âœ… Context manager thread-safe

### ğŸ”‘ Informations Critiques

**ModÃ¨le LLM** :
- Fichier : `models/zephyr-7b-beta.Q5_K_M.gguf`
- Taille : 6.8 GB
- Quantization : Q5_K_M (excellent pour RTX 4050)
- Performance : ~20-30 tokens/sec

**Base de donnÃ©es** :
- SQLite : `data/chat_history.db`
- Table : `chat_history` (7 colonnes)
- Indexes : 4 (user_id, source, timestamp, user_timestamp)

**GPU Utilisateur** :
- NVIDIA RTX 4050 (6 GB VRAM)
- Profils prÃ©vus : Performance, Balanced, CPU Fallback

**Variables d'environnement** :
- `.env` configurÃ© avec `DISCORD_TOKEN`
- `.env.example` comme template

### ğŸ“¦ Modules OpÃ©rationnels

```python
# SystÃ¨me de mÃ©moire (FONCTIONNEL)
from src.ai.memory import ConversationMemory, get_memory

memory = get_memory()  # Singleton
memory.save_interaction(user_id, source, user_input, bot_response, emotion)
history = memory.get_history(user_id, limit=10)
stats = memory.get_stats()
```

**Tests** :
```powershell
pytest tests/test_memory.py -v
# 11 passed in 0.70s âœ…
```

### âš ï¸ Ce qui N'est PAS fait

- â³ Configuration IA centralisÃ©e
- â³ Gestion LLM (chargement, gÃ©nÃ©ration)
- â³ Moteur conversationnel
- â³ Analyse Ã©motionnelle
- â³ Bot Discord
- â³ Interface GUI chat
- â³ SystÃ¨me 2FA
- â³ IntÃ©gration Unity IPC

---

## ğŸ¯ Phase 3 : Configuration IA (PROCHAINE)

### Objectif Phase 3

CrÃ©er un systÃ¨me de configuration centralisÃ© pour l'IA avec :
- âœ… Profils GPU adaptatifs
- âœ… ParamÃ¨tres LLM configurables
- âœ… System prompt personnalisable
- âœ… Validation des paramÃ¨tres

### Fichiers Ã  CrÃ©er

#### 1ï¸âƒ£ `src/ai/config.py`

**Contenu attendu** :
```python
"""
Configuration IA pour Desktop-Mate
Gestion profils GPU, paramÃ¨tres LLM, system prompt
"""

import json
from typing import Dict, Any, Optional
from dataclasses import dataclass
import logging

logger = logging.getLogger(__name__)

# Profils GPU prÃ©dÃ©finis
GPU_PROFILES = {
    "performance": {
        "n_gpu_layers": -1,      # Toutes layers sur GPU
        "n_ctx": 4096,           # Contexte max
        "n_batch": 512,          # Batch size max
        "description": "Max GPU, idÃ©al conversations courtes"
    },
    "balanced": {
        "n_gpu_layers": 35,      # 35 layers GPU (Zephyr-7B)
        "n_ctx": 2048,           # Contexte modÃ©rÃ©
        "n_batch": 256,          # Batch modÃ©rÃ©
        "description": "Ã‰quilibre GPU/CPU (DÃ‰FAUT)"
    },
    "cpu_fallback": {
        "n_gpu_layers": 0,       # CPU uniquement
        "n_ctx": 2048,
        "n_batch": 128,
        "description": "Fallback sans GPU"
    }
}

@dataclass
class AIConfig:
    """Configuration IA"""
    model_path: str
    context_limit: int
    gpu_profile: str
    temperature: float
    top_p: float
    max_tokens: int
    system_prompt: str
    
    @classmethod
    def from_json(cls, config_path: str = "data/config.json") -> "AIConfig":
        """Charge config depuis JSON"""
        # Charger config.json
        # Extraire section "ai"
        # Valider paramÃ¨tres
        # Retourner AIConfig
        pass
    
    def get_gpu_params(self) -> Dict[str, Any]:
        """Retourne paramÃ¨tres GPU du profil"""
        return GPU_PROFILES.get(self.gpu_profile, GPU_PROFILES["balanced"])
    
    def validate(self) -> bool:
        """Valide les paramÃ¨tres"""
        # VÃ©rifier model_path existe
        # VÃ©rifier gpu_profile valide
        # VÃ©rifier temperature/top_p dans [0, 1]
        # VÃ©rifier max_tokens > 0
        pass
```

**Voir dÃ©tails complets** : `PLAN_SESSION_10.md` Phase 3

#### 2ï¸âƒ£ Ã‰tendre `data/config.json`

**Ajouter section IA** :
```json
{
  "existing_config": "...",
  
  "ai": {
    "model_path": "models/zephyr-7b-beta.Q5_K_M.gguf",
    "context_limit": 2048,
    "gpu_profile": "balanced",
    "temperature": 0.7,
    "top_p": 0.9,
    "max_tokens": 512,
    "system_prompt": "Tu es Kira, l'assistante IA de Desktop-Mate. Tu es amicale, serviable et expressive. Tu peux ressentir et exprimer des Ã©motions Ã  travers tes expressions faciales. RÃ©ponds de maniÃ¨re naturelle et engageante."
  }
}
```

#### 3ï¸âƒ£ Tests : `tests/test_config.py`

**Tests Ã  crÃ©er** :
- âœ… `test_load_config()` - Chargement config.json
- âœ… `test_gpu_profiles()` - Profils GPU valides
- âœ… `test_validation()` - Validation paramÃ¨tres
- âœ… `test_default_values()` - Valeurs par dÃ©faut
- âœ… `test_invalid_profile()` - Gestion erreurs

### DurÃ©e Phase 3

**EstimÃ©e** : 1h

**DÃ©composition** :
- 30 min : `src/ai/config.py` + `GPU_PROFILES`
- 15 min : Ã‰tendre `data/config.json`
- 15 min : Tests `test_config.py`

---

## ğŸš€ Phase 4 : Model Manager (APRÃˆS Phase 3)

### Objectif Phase 4

CrÃ©er le gestionnaire LLM qui :
- âœ… Charge le modÃ¨le avec llama-cpp-python
- âœ… DÃ©tecte GPU avec pynvml
- âœ… Applique profil GPU appropriÃ©
- âœ… GÃ©nÃ¨re texte avec contexte
- âœ… GÃ¨re erreurs (OOM, model not found)

### Fichier Ã  CrÃ©er

#### `src/ai/model_manager.py`

**Contenu attendu** :
```python
"""
Model Manager pour Desktop-Mate
Gestion LLM (llama-cpp-python), GPU, gÃ©nÃ©ration texte
"""

from llama_cpp import Llama
import pynvml
from typing import Optional, Dict, List
from .config import AIConfig
import logging

logger = logging.getLogger(__name__)

class ModelManager:
    """Gestionnaire du modÃ¨le LLM"""
    
    def __init__(self, config: AIConfig):
        self.config = config
        self.model: Optional[Llama] = None
        self.gpu_available = self._check_gpu()
        
    def _check_gpu(self) -> bool:
        """DÃ©tecte GPU NVIDIA"""
        try:
            pynvml.nvmlInit()
            device_count = pynvml.nvmlDeviceGetCount()
            if device_count > 0:
                handle = pynvml.nvmlDeviceGetHandleByIndex(0)
                name = pynvml.nvmlDeviceGetName(handle)
                logger.info(f"GPU dÃ©tectÃ© : {name}")
                return True
        except Exception as e:
            logger.warning(f"GPU non dÃ©tectÃ© : {e}")
        return False
    
    def load_model(self) -> bool:
        """Charge le modÃ¨le LLM"""
        # RÃ©cupÃ©rer paramÃ¨tres GPU du profil
        # CrÃ©er Llama avec llama-cpp-python
        # GÃ©rer erreurs (OOM, fichier absent)
        pass
    
    def generate(self, prompt: str, max_tokens: Optional[int] = None) -> str:
        """GÃ©nÃ¨re texte depuis prompt"""
        # Utiliser self.model(prompt, ...)
        # Extraire texte gÃ©nÃ©rÃ©
        # GÃ©rer erreurs
        pass
    
    def get_gpu_info(self) -> Dict:
        """Retourne info GPU (VRAM, tempÃ©rature)"""
        pass
```

**Voir dÃ©tails complets** : `PLAN_SESSION_10.md` Phase 4

### Tests : `tests/test_model_manager.py`

**Tests Ã  crÃ©er** :
- âœ… `test_gpu_detection()` - DÃ©tection GPU
- âœ… `test_load_model()` - Chargement modÃ¨le
- âœ… `test_generate()` - GÃ©nÃ©ration texte
- âœ… `test_gpu_info()` - Info GPU
- âœ… `test_error_handling()` - Gestion erreurs

### DurÃ©e Phase 4

**EstimÃ©e** : 2-3h

---

## ğŸ¤ Phase 5 : Chat Engine (APRÃˆS Phase 4)

### Objectif Phase 5

CrÃ©er le moteur conversationnel unifiÃ© qui :
- âœ… Orchestre mÃ©moire + model manager
- âœ… GÃ¨re contexte conversations (historique)
- âœ… Construit prompts avec system prompt
- âœ… GÃ©nÃ¨re rÃ©ponses cohÃ©rentes
- âœ… DÃ©tecte Ã©motions (basique)

### Fichier Ã  CrÃ©er

#### `src/ai/chat_engine.py`

**Contenu attendu** :
```python
"""
Chat Engine pour Desktop-Mate
Moteur conversationnel unifiÃ© (GUI + Discord)
"""

from typing import Optional, Dict
from .model_manager import ModelManager
from .memory import ConversationMemory, get_memory
from .config import AIConfig
import logging

logger = logging.getLogger(__name__)

class ChatEngine:
    """Moteur conversationnel unifiÃ©"""
    
    def __init__(self, config: AIConfig):
        self.config = config
        self.model_manager = ModelManager(config)
        self.memory = get_memory()
        
    def start(self) -> bool:
        """DÃ©marre le moteur (charge modÃ¨le)"""
        return self.model_manager.load_model()
    
    def chat(self, user_id: str, user_input: str, source: str = "desktop") -> Dict:
        """
        GÃ©nÃ¨re rÃ©ponse pour user_input
        
        Returns:
            {
                "response": str,
                "emotion": str,
                "error": Optional[str]
            }
        """
        # 1. RÃ©cupÃ©rer historique
        # 2. Construire prompt avec system_prompt + historique + user_input
        # 3. GÃ©nÃ©rer rÃ©ponse
        # 4. DÃ©tecter Ã©motion (basique)
        # 5. Sauvegarder interaction
        # 6. Retourner dict
        pass
    
    def _build_prompt(self, history: List[Dict], user_input: str) -> str:
        """Construit prompt complet"""
        pass
    
    def _detect_emotion(self, text: str) -> str:
        """DÃ©tection Ã©motionnelle basique (keywords)"""
        # DÃ©tection simple par mots-clÃ©s
        # Version amÃ©liorÃ©e en Phase 6
        pass
```

**Voir dÃ©tails complets** : `PLAN_SESSION_10.md` Phase 5

### Tests : `tests/test_chat_engine.py`

**Tests Ã  crÃ©er** :
- âœ… `test_chat_basic()` - Conversation basique
- âœ… `test_chat_with_history()` - Avec contexte
- âœ… `test_emotion_detection()` - DÃ©tection Ã©motion
- âœ… `test_memory_integration()` - IntÃ©gration mÃ©moire
- âœ… `test_error_handling()` - Gestion erreurs

### DurÃ©e Phase 5

**EstimÃ©e** : 2-3h

---

## ğŸ“Š Ordre d'ImplÃ©mentation Chat 7

### SÃ©quence RecommandÃ©e

```
1. Phase 3 : Configuration IA (1h)
   â”œâ”€â”€ src/ai/config.py
   â”œâ”€â”€ data/config.json (Ã©tendre)
   â””â”€â”€ tests/test_config.py

2. Phase 4 : Model Manager (2-3h)
   â”œâ”€â”€ src/ai/model_manager.py
   â””â”€â”€ tests/test_model_manager.py

3. Phase 5 : Chat Engine (2-3h)
   â”œâ”€â”€ src/ai/chat_engine.py
   â””â”€â”€ tests/test_chat_engine.py

4. [OPTIONNEL] Phase 6 : Emotion Analyzer (1-2h)
   â”œâ”€â”€ src/ai/emotion_analyzer.py
   â””â”€â”€ tests/test_emotion_analyzer.py
```

**Temps total estimÃ©** :
- Phases 3-5 : 5-7h
- Avec Phase 6 : 6-9h

---

## ğŸ§ª StratÃ©gie de Tests

### Commandes Importantes

**Tester module spÃ©cifique** :
```powershell
pytest tests/test_config.py -v
pytest tests/test_model_manager.py -v
pytest tests/test_chat_engine.py -v
```

**Tester tous les tests IA** :
```powershell
pytest tests/test_memory.py tests/test_config.py tests/test_model_manager.py tests/test_chat_engine.py -v
```

**Tests avec couverture** :
```powershell
pytest --cov=src.ai tests/ --cov-report=html
```

### Validation AprÃ¨s Chaque Phase

**AprÃ¨s Phase 3** :
```powershell
pytest tests/test_config.py -v
# VÃ©rifier chargement config.json OK
```

**AprÃ¨s Phase 4** :
```powershell
pytest tests/test_model_manager.py -v
# VÃ©rifier dÃ©tection GPU OK
# VÃ©rifier chargement modÃ¨le OK (peut Ãªtre long ~30s)
```

**AprÃ¨s Phase 5** :
```powershell
pytest tests/test_chat_engine.py -v
# VÃ©rifier conversation basique OK
# VÃ©rifier sauvegarde mÃ©moire OK
```

---

## ğŸ“š RÃ©fÃ©rences Utiles

### Documentation Externe

**llama-cpp-python** :
- Docs : https://llama-cpp-python.readthedocs.io/
- ParamÃ¨tres Llama : `n_gpu_layers`, `n_ctx`, `n_batch`, `temperature`, `top_p`

**pynvml** :
- Docs : https://pypi.org/project/pynvml/
- Fonctions : `nvmlInit()`, `nvmlDeviceGetCount()`, `nvmlDeviceGetHandleByIndex()`

**SQLite Python** :
- Docs : https://docs.python.org/3/library/sqlite3.html

### Code de RÃ©fÃ©rence (Kira-Bot)

**âš ï¸ IMPORTANT** : Kira-Bot existe mais est dÃ©sorganisÃ© (score 5.7/10)

**Utilise comme rÃ©fÃ©rence UNIQUEMENT** :
- `C:\Dev\IA-chatbot\model.py` - Exemple ModelManager
- `C:\Dev\IA-chatbot\config.py` - Exemple configuration
- `C:\Dev\IA-chatbot\memory.py` - Notre version est meilleure !

**NE PAS copier-coller** : Adapter les concepts, rÃ©Ã©crire proprement

---

## ğŸ”§ Configuration Utilisateur

### Variables d'Environnement

**Fichier** : `.env` (configurÃ©)
```
DISCORD_TOKEN=<configurÃ©_par_utilisateur>
TOTP_SECRET=<sera_gÃ©nÃ©rÃ©_en_phase_10>
```

### MatÃ©riel

**GPU** : NVIDIA RTX 4050 (6 GB VRAM)  
**Profil recommandÃ©** : `balanced` (dÃ©faut)  
**Performance attendue** : 20-30 tokens/sec

---

## ğŸš¨ Points d'Attention

### 1. Chargement ModÃ¨le (Phase 4)

**âš ï¸ Peut Ãªtre lent** : Chargement initial ~20-30s pour 6.8 GB

**Solutions** :
- Afficher message "Chargement modÃ¨le..."
- Logger progression
- GÃ©rer timeout

### 2. Gestion MÃ©moire GPU (Phase 4)

**âš ï¸ Risque OOM** : Si profil trop Ã©levÃ© pour GPU

**Solutions** :
- Commencer avec profil `balanced`
- DÃ©tecter erreur OOM
- Fallback automatique vers `cpu_fallback`
- Logger Ã©vÃ©nement

### 3. Contexte Conversations (Phase 5)

**âš ï¸ Limite contexte** : 2048 tokens (profil balanced)

**Solutions** :
- Limiter historique Ã  10 derniers messages
- RÃ©sumer vieux messages (Phase future)
- Afficher warning si contexte plein

### 4. Tests Longs (Phase 4-5)

**âš ï¸ Tests avec LLM** : GÃ©nÃ©ration texte peut prendre 5-10s

**Solutions** :
- Utiliser `@pytest.mark.slow` pour tests LLM
- Permettre `pytest -m "not slow"` pour tests rapides
- Mock pour tests unitaires, intÃ©gration pour tests lents

---

## ğŸ“– Documentation Ã  Mettre Ã  Jour

### AprÃ¨s CHAQUE Phase

**OBLIGATOIRE** :
- âœ… `docs/INDEX.md` - Ajouter nouveaux fichiers
- âœ… `docs/README.md` - Si architecture modifiÃ©e
- âœ… `README.md` (racine) - Si fonctionnalitÃ© majeure
- âœ… `docs/sessions/session_10_ai_chat/README.md` - Progression phases

**SYSTÃˆME CRITIQUE** :
Suivre `.github/instructions/copilot-instructions.instructions.md`

**RÃ¨gle d'or** :
> "L'utilisateur ne devrait JAMAIS avoir Ã  demander si la documentation est Ã  jour"

---

## ğŸ¯ Objectif Final Chat 7

Ã€ la fin du Chat 7, Desktop-Mate devrait avoir :

âœ… Configuration IA centralisÃ©e et testÃ©e  
âœ… ModÃ¨le LLM chargeable avec profils GPU  
âœ… Moteur conversationnel fonctionnel  
âœ… GÃ©nÃ©ration de rÃ©ponses avec contexte  
âœ… Sauvegarde automatique des conversations  
âœ… DÃ©tection Ã©motionnelle basique  
âœ… Tests complets (15-20 tests)  
âœ… Documentation Ã  jour  

**PrÃªt pour** :
- Phase 7 : Bot Discord (Chat 8)
- Phase 8 : GUI Chat (Chat 8)

---

## ğŸ’¡ Conseils pour l'IA (Toi)

### Workflow RecommandÃ©

**Pour chaque phase** :
1. âœ… Lire dÃ©tails dans `PLAN_SESSION_10.md`
2. âœ… Expliquer ce que tu vas faire
3. âœ… CrÃ©er les fichiers Python
4. âœ… CrÃ©er les tests
5. âœ… ExÃ©cuter les tests
6. âœ… Corriger les erreurs
7. âœ… **METTRE Ã€ JOUR DOCUMENTATION**
8. âœ… Afficher rÃ©capitulatif phase

**JAMAIS dire "TerminÃ©" sans** :
- âœ… Tests passants
- âœ… Documentation mise Ã  jour
- âœ… RÃ©capitulatif affichÃ©

### Communication avec Utilisateur

**Rappels** :
- ğŸ‡«ğŸ‡· Toujours en franÃ§ais
- ğŸ“– Expliquer clairement les concepts techniques
- âš ï¸ Demander confirmation avant changements majeurs
- ğŸ“ L'utilisateur n'est pas expert â†’ ÃŠtre pÃ©dagogue

### Gestion du Temps

**Phases longues (4-5)** :
- DÃ©couper en sous-tÃ¢ches
- Afficher progression rÃ©guliÃ¨rement
- Permettre pauses si demandÃ©es

**Tests** :
- Ne pas attendre la fin pour tester
- Tester aprÃ¨s chaque mÃ©thode critique
- Corriger au fur et Ã  mesure

---

## ğŸ”— Liens Rapides

**Documentation Session 10** :
- `docs/sessions/session_10_ai_chat/PLAN_SESSION_10.md` â† **CRITIQUE**
- `docs/sessions/session_10_ai_chat/README.md`

**Transition Chat 6** :
- `docs/chat_transitions/chat_6_session_10_phases_1_2/CURRENT_STATE.md`
- `docs/chat_transitions/chat_6_session_10_phases_1_2/CHAT_SUMMARY.md`

**Instructions Projet** :
- `.github/instructions/copilot-instructions.instructions.md`

**Code Existant** :
- `src/ai/memory.py` â† Exemple qualitÃ© attendue
- `tests/test_memory.py` â† Exemple tests complets

---

## ğŸŠ Message de Motivation

Chat 6 a posÃ© des **bases solides** ! ğŸ‰

L'architecture est propre, la mÃ©moire fonctionne parfaitement (11/11 tests âœ…), et le modÃ¨le LLM est prÃªt.

**Chat 7 va donner vie Ã  Kira** en lui permettant de :
- ğŸ§  Penser (Model Manager)
- ğŸ’¬ Converser (Chat Engine)
- ğŸ­ Ressentir (Emotion basique)

**Tu es prÃªt Ã  crÃ©er la partie la plus excitante du projet ! ğŸš€**

---

**Bon dÃ©veloppement Chat 7 ! Tu as toutes les infos nÃ©cessaires ! ğŸ­âœ¨**
